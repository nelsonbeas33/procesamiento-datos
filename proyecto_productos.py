# -*- coding: utf-8 -*-
"""proyecto modelo de negocios.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G6-n6BNL2xeAvHNlON74KYPCPiSOrK9u
"""

# Paso 1: Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Paso 2: Leer el archivo CSV
import pandas as pd

# Ruta al archivo CSV en Google Drive
file_products = '/content/drive/MyDrive/Datos Masivos/products.csv'
file_pasillos = '/content/drive/MyDrive/Datos Masivos/aisles.csv'
file_departamentos = '/content/drive/MyDrive/Datos Masivos/departments.csv'

# Leer el archivo CSV
df_products = pd.read_csv(file_products)
df_pasillos = pd.read_csv(file_pasillos)
df_departamentos = pd.read_csv(file_departamentos)

df = pd.merge(df_products, df_departamentos, on='department_id', how='left')
df = pd.merge(df, df_pasillos, on='aisle_id', how='left')

# Mostrar las primeras filas del DataFrame
print(df.head(1))

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download("stopwords")
nltk.download("wordnet")

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

# Lista de palabras clave a eliminar
palabras_clave_eliminar = ["pack", "mix", "promo"]

# Lista de palabras clave para marcar productos bajos en grasa o de dieta
palabras_clave_bajo_en_grasa = [
    "low fat", "fat free", "diet", "light",
    "reduced fat", "zero fat", "low calorie", "sugar free"
]

def clean_text(text):
    # Eliminar caracteres especiales y números
    text = text.lower()
    text = re.sub(r"[^a-zA-Z]", " ", text.lower())

    # Tokenizar y lematizar
    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]

    # Unir tokens en un solo texto
    text_limpio = " ".join(tokens)

    # Verificar si se retiró alguna palabra clave (pack, mix, promo)
    se_retiro_palabra_clave = any(palabra in text for palabra in palabras_clave_eliminar)

    # Eliminar palabras clave (pack, mix, promo) del texto
    for palabra in palabras_clave_eliminar:
        text_limpio = text_limpio.replace(palabra, "").strip()

    # Verificar si el producto es bajo en grasa o de dieta
    es_bajo_en_grasa = any(palabra in text for palabra in palabras_clave_bajo_en_grasa)

    return text_limpio, se_retiro_palabra_clave, es_bajo_en_grasa

# Aplicar la función a la columna 'product_name'
df[["cleaned_product_name", "se_retiro_palabra_clave", "es_bajo_en_grasa"]] = df["product_name"].apply(
    lambda x: pd.Series(clean_text(x))
)

# Mostrar las primeras filas del DataFrame
print(df.head())

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-mpnet-base-v2")  # Más preciso que MiniLM
product_embeddings = model.encode(df["cleaned_product_name"], show_progress_bar=True)

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")  # Más rápido y ligero
product_embeddings = model.encode(df["cleaned_product_name"], show_progress_bar=True)

# Importar bibliotecas necesarias
from sklearn.preprocessing import OneHotEncoder

# Codificar variables categóricas (pasillo y departamento)
encoder = OneHotEncoder(sparse_output=False, drop="first")  # Usamos drop="first" para evitar multicolinealidad
categorical_features = encoder.fit_transform(df[["department", "aisle"]])

# Combinar embeddings, variables categóricas y nuevas variables
features = np.hstack([
    product_embeddings,
    categorical_features,
    df[["se_retiro_palabra_clave", "es_bajo_en_grasa"]]
])

# Importar bibliotecas necesarias
from sklearn.preprocessing import StandardScaler

# Normalizar los datos
scaler = StandardScaler()
features_normalized = scaler.fit_transform(features)

from sklearn.cluster import MiniBatchKMeans

n_clusters = 10000  # Aún puedes usar un número alto de clusters
mbkmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000)
df["cluster"] = mbkmeans.fit_predict(features_normalized)

# Importar bibliotecas necesarias
from sklearn.metrics import silhouette_score

# Evaluar el clustering
silhouette_avg = silhouette_score(features_normalized, df["cluster"])
print(f"Silhouette Score: {silhouette_avg}")

# Importar bibliotecas necesarias
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reducción de dimensionalidad con PCA
pca = PCA(n_components=2)  # Reducir a 2 dimensiones para visualización
features_2d = pca.fit_transform(features_normalized)

# Crear un DataFrame para visualización
df_visualization = pd.DataFrame(features_2d, columns=["Componente 1", "Componente 2"])
df_visualization["cluster"] = df["cluster"]

# Graficar los clusters
plt.figure(figsize=(10, 6))
plt.scatter(df_visualization["Componente 1"], df_visualization["Componente 2"], c=df_visualization["cluster"], cmap="viridis", s=50)
plt.title("Clustering de Productos")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.colorbar(label="Cluster")
plt.show()

# Asegurarse de que el DataFrame tenga todas las columnas necesarias
print(df.columns)  # Verificar las columnas disponibles

# Guardar el DataFrame en un archivo CSV
df.to_csv("productos_con_clusters.csv", index=False, encoding="utf-8")

# Mensaje de confirmación
print("Archivo CSV guardado correctamente: 'productos_con_clusters.csv'")